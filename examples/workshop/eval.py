"""Evaluate models using various metrics."""

import argparse
import gzip
import json
import os
import pickle
from typing import Literal

from huggingface_hub import hf_hub_download

from xtalmet.evaluator import Evaluator


def eval(
	model: Literal["adit", "cdvae", "chemeleon", "diffcsp", "diffcsppp", "mattergen"],
	screen: Literal["none", "ehull"],
	metric: Literal["uniqueness", "novelty", "both"],
) -> None:
	"""Evaluate crystals generated by a model after optional screening.

	Args:
		model (str): Model used to generate crystals.
		screen (str): Screening method.
		metric (str): Metric(s) to evaluate.
	"""
	# Create directory to save results and intermediate files
	dir_save = os.path.join(os.path.dirname(__file__), "results", "mp20", model)
	os.makedirs(dir_save, exist_ok=True)
	# Load existing results if available
	if os.path.exists(os.path.join(dir_save, "results.json")):
		with open(os.path.join(dir_save, "results.json")) as f:
			results = json.load(f)
	else:
		results = {}
	for distance in ["smat", "comp", "wyckoff", "magpie", "pdd", "amd"]:
		if distance not in results:
			results[distance] = {}
	# Download generated crystals
	path = hf_hub_download(
		repo_id="masahiro-negishi/xtalmet",
		filename=f"mp20/model/{model}.pkl.gz",
		repo_type="dataset",
		revision="v0.1.1",
	)
	with gzip.open(path, "rb") as f:
		gen_xtals = pickle.load(f)
	evaluator = Evaluator(gen_xtals)
	# Evaluation
	screen_arg = None if screen == "none" else screen
	for distance in ["smat", "comp", "wyckoff", "magpie", "pdd", "amd"]:
		if metric == "uniqueness":
			uni, uni_times = evaluator.uniqueness(distance, screen_arg, dir_save, True)
			if screen in results[distance]:
				results[distance][screen].update(uni_times | {"uniqueness": uni})
			else:
				results[distance][screen] = uni_times | {"uniqueness": uni}
		elif metric == "novelty":
			nov, nov_times = evaluator.novelty(
				"mp20", distance, screen_arg, dir_save, True
			)
			if screen in results[distance]:
				results[distance][screen].update(nov_times | {"novelty": nov})
			else:
				results[distance][screen] = nov_times | {"novelty": nov}
		else:  # both
			uni, uni_times = evaluator.uniqueness(distance, screen_arg, dir_save, True)
			nov, nov_times = evaluator.novelty(
				"mp20", distance, screen_arg, dir_save, True
			)
			if screen in results[distance]:
				results[distance][screen].update(
					uni_times | nov_times | {"uniqueness": uni, "novelty": nov}
				)
			else:
				results[distance][screen] = (
					uni_times | nov_times | {"uniqueness": uni, "novelty": nov}
				)
	with open(os.path.join(dir_save, "results.json"), "w") as f:
		json.dump(results, f, indent=4, sort_keys=True)


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument(
		"--model",
		type=str,
		required=True,
		choices=[
			"adit",
			"cdvae",
			"chemeleon",
			"diffcsp",
			"diffcsppp",
			"mattergen",
		],
		help="Model used to generate crystals.",
	)
	parser.add_argument(
		"--screen",
		type=str,
		required=True,
		choices=["none", "ehull"],
		help="Screening method.",
	)
	parser.add_argument(
		"--metric",
		type=str,
		required=True,
		choices=["uniqueness", "novelty", "both"],
		help="Metric(s) to evaluate.",
	)
	args = parser.parse_args()
	eval(**vars(args))
